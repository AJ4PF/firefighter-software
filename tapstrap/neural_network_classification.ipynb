{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the goal of this file is to attempt doing this with pure accelerometer data, along with rolling feature extraction. \n",
    "## this data will be fed into a neural network indicating the kind of gesture is being done. \n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from tools import feature_extraction, table\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore all warning messages  \n",
    "\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    # the tap strap has 5 xyz accelerometers\n",
    "    # and a imu on the thumb. The \n",
    "    data = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    # Break payload into separate columns\n",
    "    if all(len(i) == 15 for i in df['payload']): # case we are loading in data from the general tap strap \n",
    "        df[['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z', \n",
    "            'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z']] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    elif all(len(i) == 6 for i in df['payload']): # case we are loading in imu data for the thumb \n",
    "        df[['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll',\n",
    "            ]] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    elif all(len(i) == 21 for i in df['payload']): # case we are loading in merged/interpolated data \n",
    "        df[['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll', 'thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z', \n",
    "            'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z' ,\n",
    "            ]] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    else:\n",
    "        print(\"Some payloads do not have the expected length of 15 or 6.\")\n",
    "    \n",
    "    # Drop the original 'payload' column\n",
    "    df = df.drop(columns=['payload'])\n",
    "    df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"./training_data/data/Still2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_feature_extraction(new_dataframe, use_label, interpolated = False):\n",
    "    # supress warnings\n",
    "    features = ['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z',\n",
    "                'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z']\n",
    "\n",
    "    fingers = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "    if interpolated:\n",
    "        imu_features = ['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll']\n",
    "\n",
    "        features = imu_features +  ['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z',\n",
    "                'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z'] \n",
    "\n",
    "    # Average acceleration per axis\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df = new_dataframe[features]\n",
    "    rolling_data_frames = []\n",
    "    # print(\"num nans:\", new_df.isnull().sum().sum())\n",
    "    cols = new_df.columns.tolist()\n",
    "    window_size = 15\n",
    "    for feature in features:\n",
    "        new_df['{}_rolling_mean'.format(feature)] = new_df[feature].rolling(window=window_size).mean()\n",
    "        new_df['{}_rolling_std'.format(feature)] = new_df[feature].rolling(window=window_size).std()\n",
    "        new_df['{}_rolling_variance'.format(feature)] = new_df[feature].rolling(window=window_size).var()\n",
    "        new_df['{}_rolling_derivative'.format(feature)] = new_df['{}'.format(feature)].diff()\n",
    "\n",
    "    # for feature in features:\n",
    "    #     rolling_data_frames.append(new_df[feature].rolling(window=window_size).mean().rename('{}_rolling_mean'.format(feature)))\n",
    "    #     rolling_data_frames.append(new_df[feature].rolling(window=window_size).std().rename('{}_rolling_std'.format(feature)))\n",
    "    #     rolling_data_frames.append(new_df[feature].rolling(window=window_size).var().rename('{}_rolling_variance'.format(feature)))\n",
    "    #     rolling_data_frames.append(new_df[feature].diff().rename('{}_rolling_derivative'.format(feature)))\n",
    "\n",
    "    # Concatenate all rolling features into a new DataFrame\n",
    "    # rolling_df = pd.concat(rolling_data_frames, axis=1)\n",
    "    # rolling_df = rolling_df[window_size:]\n",
    "    ## insert here \n",
    "    if(use_label):\n",
    "        new_df['label'] = new_dataframe['label'][0]  # this will either be 0 or 1\n",
    "    # drop first n rows where n is the window size\n",
    "    new_df = new_df[window_size:]\n",
    "\n",
    "    # table(data_df)\n",
    "    # print('SHAPE:', data_df.shape)\n",
    "    return new_df\n",
    "\n",
    "# attempt to do the feature extraction with the interpolated data \n",
    "dir_list = os.listdir(\"./training_data/data_2\")\n",
    "\n",
    "num_still_folders = len([i for i in dir_list if \"still\" in i])\n",
    "num_turn_folders = len([i for i in dir_list if \"turn\" in i])\n",
    "num_lever_folders = len([i for i in dir_list if \"lever\" in i])\n",
    "gesture_folders = [('lever', num_lever_folders), ('turn', num_turn_folders), ('still', num_still_folders) ] # will refcator this into a simpler loop\n",
    "print(\"Gesture Folders: \", gesture_folders)\n",
    "acc = pd.DataFrame()\n",
    "count = 0\n",
    "# '../../data/Still2/imu_data.json'\n",
    "list_of_dataframes = []\n",
    "for gesture_name,number_items in gesture_folders:\n",
    "    print('looking at ',gesture_name, \"with \", number_items, \"number of folders\" )\n",
    "    for  i in range(number_items):\n",
    "        # load the data\n",
    "        file_name = str(f'training_data/data_2/{gesture_name}{i}/merged_data.json')\n",
    "        ndf = load_data(file_name)\n",
    "        features = rolling_feature_extraction(ndf, use_label = True, interpolated=True)\n",
    "        list_of_dataframes.append(features)\n",
    "        count += 1\n",
    "        acc = pd.concat([acc, features], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accel_df = load_data('./training_data/data_2/Turn3/accel_data.json')\n",
    "d = rolling_feature_extraction(test_accel_df, use_label=True)\n",
    "\n",
    "print(d.head(1))\n",
    "\n",
    "# get first 2 seconds of data\n",
    "# acc\n",
    "# get number of of columns\n",
    "# print(acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Assuming 'acc' is your DataFrame with features and labels\n",
    "\n",
    "# # Drop label column if present and store it separately\n",
    "# # labels = acc.pop('label').values\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# # scaled_data = scaler.fit_transform(acc)\n",
    "# scaled_data = acc\n",
    "# # \n",
    "# X = final_data.drop('label', axis=1)\n",
    "# y = final_data['label']\n",
    "\n",
    "# # Reshape data for LSTM input: (samples, time steps, features)\n",
    "# n_samples, n_features = scaled_data.shape\n",
    "# time_steps = 15  # You might need to adjust this based on your data characteristics\n",
    "# # X = scaled_data.reshape((n_samples, time_steps, n_features))\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(Dense(units=256, activation='relu'))\n",
    "# # model.add(Dense(units=1, activation='sigmoid'))  # Assuming binary classification (sigmoid activation)\n",
    "# model.add(Dense(3, activation='softmax'))  # Assuming binary classification (sigmoid activation)\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "# import reshape \n",
    "# from tensorflow.keras.layers import Masking\n",
    "# import Reshape for tensorflow\n",
    "from tensorflow.keras.layers import Reshape\n",
    "    # print sample\n",
    "    # print(\"sample: \", sample.head(5))\n",
    "    # # Load data from the file\n",
    "    # ndf = load_data(file_path)\n",
    "\n",
    "    # # Extract rolling features\n",
    "    # features = rolling_feature_extraction(ndf)\n",
    "\n",
    "    # Separate features and labels\n",
    "    # labels = sample.pop('label').values\n",
    "\n",
    "    # Standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # scaled_data = scaler.fit_transform(sample)\n",
    "\n",
    "\n",
    "    # Find the maximum sequence length in your dataset\n",
    "    # max_sequence_length = max(len(seq) for seq in scaled_data)\n",
    "    # print(\"max_sequence_length: \", max_sequence_length)\n",
    "    # Pad or truncate sequences to the maximum length\n",
    "    # padded_data = tf.keras.preprocessing.sequence.pad_sequences(scaled_data, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
    "    # padded_data = scaled_data\n",
    "\n",
    "    # Reshape data for LSTM input: (samples, time steps, features)\n",
    "    # n_samples, n_features = padded_data.shape\n",
    "    # time_steps = n_features\n",
    "    # X = padded_data.reshape((1, time_steps, n_samples))\n",
    "    # truncated_data = scaled_data.reshape((1, 50, 105)) \n",
    "n_features = 105\n",
    "n_timesteps = 10\n",
    "def create_lstm_model():\n",
    "    # model.add(Masking(mask_value=0.0, input_shape=input_shape))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='tanh', return_sequences=True, input_shape=(n_timesteps, n_features)))\n",
    "    # model.add(Reshape((n_timesteps, 64)))  # Reshape for dense layer\n",
    "    model.add(Reshape((n_timesteps, 64)))  # Reshape for dense layer\n",
    "    model.add(Dense(units=128, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model()\n",
    "for sample in list_of_dataframes[:2]:\n",
    "\n",
    "    # take only the first 50 rows of data\n",
    "    truncated_data = sample[:n_timesteps]\n",
    "    print(\"truncated_data: \", truncated_data.shape)\n",
    "    # print(truncated_data.head(1))\n",
    "    # Assuming 'label' is your target variable\n",
    "    X = truncated_data.drop('label', axis=1)\n",
    "    y = truncated_data['label']\n",
    "    X = X.values.reshape((X.shape[0], n_timesteps, n_features))\n",
    "    # Reshape X for LSTM input: (samples, time steps, features)\n",
    "    # X = X.reshape((1, n_timesteps, n_features))\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the LSTM model\n",
    "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy for {i}: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Model parameters\n",
    "\n",
    "n_features = 105  \n",
    "n_timesteps = 20\n",
    "\n",
    "def create_lstm_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='tanh', return_sequences=True, input_shape=(n_timesteps, 104)))\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh')) # Add a second LSTM layer\n",
    "\n",
    "    # dense layer \n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dense(1, activation='softmax')) # Output layer\n",
    "    # model.compile(loss='sarse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model  \n",
    "lstm_model = create_lstm_model()\n",
    "\n",
    "# Loop through dataframes\n",
    "# take only the 100-105 rows of data\n",
    "for i, sample in enumerate(list_of_dataframes):\n",
    "    # normalize the dataframe \n",
    "    # normalize  the dataframe\n",
    "    normalzed_df = sample.copy()\n",
    "    # for feature_name in sample.columns:\n",
    "    #     if feature_name != 'label':\n",
    "    #         max_value = sample[feature_name].max()\n",
    "    #         min_value = sample[feature_name].min()\n",
    "    #         normalzed_df[feature_name] = (sample[feature_name] - min_value) / (max_value - min_value)\n",
    "\n",
    "    sample = normalzed_df\n",
    "    # sample = (sample - sample.mean()) / (sample.max() - sample.min())\n",
    "    # Preprocess data\n",
    "    # truncated_data = sample[:50] \n",
    "    # print(\"truncated_data: \", truncated_data.shape)\n",
    "    # X = truncated_data.drop('label', axis=1).values\n",
    "    # X = X.reshape(X.shape[0], n_timesteps,  int(X.shape[1]/n_timesteps))\n",
    "    # y = truncated_data['label']\n",
    "    # print(\"x shape: \", X.shape)\n",
    "    scaler = StandardScaler()\n",
    "    # scaled_data = scaler.fit_transform(sample)\n",
    "    # sample = pd.DataFrame(scaled_data, columns=sample.columns)\n",
    "    \n",
    "    # Get input data\n",
    "    X = sample.drop('label', axis=1).values\n",
    "\n",
    "    # scaler.fit(X)\n",
    "    # scaled_x = pd.DataFrame(scaler.transform(X), columns=sample.columns[:-1])\n",
    "    # X = scaled_x.values\n",
    "    num_samples = X.shape[0]\n",
    "    print(\"num_samples: \", num_samples)\n",
    "\n",
    "    # Calculate number of complete timesteps\n",
    "    num_complete_timesteps = math.floor(num_samples / n_timesteps)\n",
    "    print(\"num_complete_timesteps: \", num_complete_timesteps)\n",
    "    print(\"num features: \", X.shape[1])\n",
    "\n",
    "    # Drop any leftover samples\n",
    "    sample = sample[:num_complete_timesteps * n_timesteps]\n",
    "    # X = X[:num_complete_timesteps * n_timesteps]\n",
    "\n",
    "    # # Reshape \n",
    "    # X = X.reshape(num_complete_timesteps, n_timesteps, n_features)\n",
    "    # y = sample['label'][:num_complete_timesteps * n_timesteps]\n",
    "    # print(\"x shape after: \", X.shape)\n",
    "    # # reshape y \n",
    "    # y = y.values.reshape(num_complete_timesteps, n_timesteps, 1)\n",
    "    # print(\"y shape after: \", y.shape)\n",
    "\n",
    "    print(\"label: \", sample['label'].head(1))\n",
    "\n",
    "    df = sample.drop('label', axis=1)\n",
    "    #print the label \n",
    "    df = df.values.reshape(num_complete_timesteps, n_timesteps, n_features)\n",
    "    # One-hot encode target data \n",
    "    # gestures = ['wave', 'clap', 'punch']\n",
    "    # y = np.zeros((len(y), len(gestures)))\n",
    "    # for i, gesture in enumerate(y):\n",
    "    #     y[i, gestures.index(gesture)] = 1 \n",
    "\n",
    "    # Now split into X and y\n",
    "    X = df[:, :, :-1] \n",
    "    y = df[:, -1, -1]\n",
    "    print(\"x shape after: \", X.shape)\n",
    "    print(\"y shape after: \", y.shape)\n",
    "\n",
    "# Split data into train/test\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model \n",
    "    lstm_model.fit(X_train, y_train, epochs=2, batch_size=10, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate model\n",
    "    loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "    print(f'Accuracy for DF {i}: {accuracy*100:.2f}%')\n",
    "\n",
    "    # run a prediction on the model\n",
    "\n",
    "    print(\"running a prediction on the model\")\n",
    "    prediction = lstm_model.predict(X_test)\n",
    "    print(\"prediction: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = load_data('./training_data/data_2/Turn3/merged_data.json')\n",
    "sample = rolling_feature_extraction(load_data('./training_data/data_2/Lever2/merged_data.json'), use_label=True, interpolated=True)\n",
    "print(\"num_complete_timesteps: \", num_complete_timesteps)\n",
    "print(\"num features: \", X.shape[1])\n",
    "num_complete_timesteps = math.floor(num_samples / n_timesteps)\n",
    "\n",
    "X = sample.drop('label', axis=1).values\n",
    "\n",
    "# scaler.fit(X)\n",
    "# scaled_x = pd.DataFrame(scaler.transform(X), columns=sample.columns[:-1])\n",
    "# X = scaled_x.values\n",
    "num_samples = X.shape[0]\n",
    "print(\"num_samples: \", num_samples)\n",
    "\n",
    "# Calculate number of complete timesteps\n",
    "num_complete_timesteps = math.floor(num_samples / n_timesteps)\n",
    "# print(\"num_complete_timesteps: \", num_complete_timesteps)\n",
    "print(\"num features: \", X.shape[1])\n",
    "\n",
    "# Drop any leftover samples\n",
    "sample = sample[:num_complete_timesteps * n_timesteps]\n",
    "df = sample.drop('label', axis=1)\n",
    "#print the label \n",
    "df = df.values.reshape(num_complete_timesteps, n_timesteps, n_features)\n",
    "# Now split into X and y\n",
    "X = df[:, :, :-1] \n",
    "y = df[:, -1, -1]\n",
    "# print(\"x shape after: \", X.shape)\n",
    "# print(\"y shape after: \", y.shape)\n",
    "\n",
    "prediction = lstm_model.predict(X)\n",
    "print(\"prediction: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # new_df['{}_rolling_avg_accel_mag'.format(feature)] = new_df['{}_rolling_derivative'.format(feature)].rolling(window=window_size).mean()\n",
    "    # label = new_dataframe['label']\n",
    "    # label_value = label[0]\n",
    "    # table(new_dataframe)\n",
    "    # avg_accel = new_dataframe[features].mean()\n",
    "    # # print(avg_accel)\n",
    "\n",
    "    # # calculate the average jerk per axis\n",
    "    # avg_jerk = new_dataframe[features].diff().mean()\n",
    "\n",
    "    # # calculate the variance per axis\n",
    "    # variance = new_dataframe[features].var()\n",
    "    \n",
    "    # # print(\"JERK \\n\", avg_jerk)\n",
    "\n",
    "    # # Standard deviation per axis\n",
    "    # std_dev_accel = new_dataframe[features].std()\n",
    "    # # print(std_dev_accel)\n",
    "    # skew = new_dataframe[features].skew()\n",
    "    # kurtosis = new_dataframe[features].kurtosis()\n",
    "    # # Average absolute difference per axis\n",
    "    # avg_abs_diff_accel = new_dataframe[features].diff().abs().mean()\n",
    "    # # print(avg_abs_diff_accel)\n",
    "\n",
    "    # # Initialize dictionary to hold results\n",
    "    # avg_accel_mag = {}\n",
    "    # # Loop over each finger and calculate the average acceleration magnitude\n",
    "    # for finger in fingers:\n",
    "    #     avg_accel_mag[finger] = ((new_dataframe[[f'{finger}_x', f'{finger}_y', f'{finger}_z']] ** 2).sum(axis=1) ** 0.5).mean()\n",
    "    # # rename each key to be avg_accel_mag_{finger}\n",
    "    # # print(avg_accel_mag)\n",
    "    # # Time between peaks per axis\n",
    "    # time_between_peaks = {}\n",
    "    # for feature in features:\n",
    "    #     peaks, _ = find_peaks(new_dataframe[feature])\n",
    "    #     # check that there are peaks\n",
    "    #     if len(peaks) > 1:\n",
    "    #         time_between_peaks[feature] = np.diff(peaks).mean()\n",
    "    #         # if np.isnan(time_between_peaks[feature]):\n",
    "    #         #     time_between_peaks[feature] = 0\n",
    "    #     else:\n",
    "    #         time_between_peaks[feature] = 0\n",
    "    # # print(time_between_peaks)\n",
    "\n",
    "    # variance_dict = {f'variance_{k}': v for k, v in variance.items()}\n",
    "\n",
    "    # # rename each key to be avg_accel_{finger}\n",
    "    # avg_accel_dict = {f'avg_accel_{k}': v for k, v in avg_accel.items()}\n",
    "    # # print(\"Average acceleration\\n\", avg_accel_dict)\n",
    "    # # rename each key to be std_dev_accel_{finger}\n",
    "    # std_dev_accel_dict = {f'std_dev_accel_{k}': v for k, v in std_dev_accel.items()}\n",
    "    # # print(\"Accel Std Dev.\\n\", std_dev_accel_dict)\n",
    "    # # rename each key to be avg_abs_diff_accel_{finger}\n",
    "    # avg_abs_diff_accel_dict = {f'avg_abs_diff_accel_{k}': v for k, v in avg_abs_diff_accel.items()}\n",
    "    # # print(\"Average Accel Absolute Diff\\n\", avg_abs_diff_accel_dict)\n",
    "    # time_between_peaks_dict = {f'time_between_peaks_{k}': v for k, v in time_between_peaks.items()}\n",
    "    # # print(\"Time B/W Peaks\\n\", time_between_peaks_dict)\n",
    "    # avg_accel_mag_dict = {f'avg_accel_mag_{k}': v for k, v in avg_accel_mag.items()}\n",
    "    # # print(\"Average Accel mag\\n\", avg_accel_mag_dict\n",
    "    # avg_jerk_dict = {f'avg_jerk_{k}': v for k,v in avg_jerk.items()}\n",
    "    # kurtosis_dict = {f'kurtosis_{k}': v for k, v in kurtosis.items()}\n",
    "    # skew_dict = {f'skew_{k}': v for k, v in skew.items()}\n",
    "\n",
    "    # # Convert dictionaries to DataFrames\n",
    "    # avg_accel_df = pd.DataFrame(avg_accel_dict, index=[0])\n",
    "    # std_dev_accel_df = pd.DataFrame(std_dev_accel_dict, index=[0])\n",
    "    # avg_abs_diff_accel_df = pd.DataFrame(avg_abs_diff_accel_dict, index=[0])\n",
    "    # time_between_peaks_df = pd.DataFrame(time_between_peaks_dict, index=[0])\n",
    "    # # replace all NaN values with 0\n",
    "    # time_between_peaks_df = time_between_peaks_df.fillna(0)\n",
    "    # avg_accel_mag_df = pd.DataFrame(avg_accel_mag_dict, index=[0])\n",
    "    # avg_jerk_df = pd.DataFrame(avg_jerk_dict, index=[0])\n",
    "    # kurtosis_df = pd.DataFrame(kurtosis_dict, index=[0])\n",
    "    # skew_df = pd.DataFrame(skew_dict, index=[0])\n",
    "    # variance_df = pd.DataFrame(variance_dict, index=[0])\n",
    "\n",
    "    # # filter out any NaN values\n",
    "    # # replace all NaN values with 0\n",
    "    # time_between_peaks_df = time_between_peaks_df.fillna(0)\n",
    "    # # time_between_peaks_df = avg_accel_df.dropna(axis=1)\n",
    "    # # Concatenate DataFrames\n",
    "    # data_df = pd.concat([variance_df,skew_df,kurtosis_df,avg_accel_df, std_dev_accel_df, avg_abs_diff_accel_df, time_between_peaks_df, avg_accel_mag_df, avg_jerk_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
